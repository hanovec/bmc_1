# Tento soubor je app.py pro nasazen√≠ na Streamlit Cloud

import streamlit as st
import google.generativeai as genai
import os
import uuid
import json
import re
import time
import textwrap

# ==============================================================================
# BLOK 1: KONFIGURACE A INICIALIZACE
# ==============================================================================

# --- Konfigurace str√°nky Streamlit ---
st.set_page_config(page_title="BMC Navig√°tor", layout="wide")

# --- Naƒçten√≠ API kl√≠ƒçe ze Streamlit Secrets ---
try:
    GOOGLE_API_KEY = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=GOOGLE_API_KEY)
except (KeyError, FileNotFoundError):
    st.error("API kl√≠ƒç pro Google Gemini nen√≠ nastaven v 'Secrets' va≈°√≠ Streamlit aplikace!")
    st.info("P≈ôejdƒõte do nastaven√≠ va≈°√≠ aplikace na share.streamlit.io, kliknƒõte na 'Manage app', pot√© 'Settings' -> 'Secrets' a vlo≈æte sv≈Øj kl√≠ƒç ve form√°tu: GOOGLE_API_KEY = \"vas_klic_sem\"")
    st.stop()

# --- Konfigurace Modelu ---
PRIORITY_MODEL_STEMS = ["gemini-2.5-flash-preview-05-20", "gemini-1.5-flash-latest", "gemini-1.5-pro-latest", "gemini-pro"]
GENERATION_CONFIG = {"temperature": 0.7, "top_p": 0.95, "max_output_tokens": 65536}
model = None

@st.cache_resource
def load_model():
    model_name_to_use = None
    try:
        available_models = [m for m in genai.list_models() if "generateContent" in m.supported_generation_methods]
        for model_stem in PRIORITY_MODEL_STEMS:
            found_model = next((m for m in available_models if model_stem in m.name and 'vision' not in m.name.lower()), None)
            if found_model: model_name_to_use = found_model.name; break
        if not model_name_to_use: st.error("Nebyl nalezen ≈æ√°dn√Ω z prioritn√≠ch model≈Ø."); return None
        return genai.GenerativeModel(model_name=model_name_to_use, generation_config=GENERATION_CONFIG)
    except Exception as e:
        st.error(f"KRITICK√Å CHYBA p≈ôi inicializaci modelu: {e}")
        return None

model = load_model()

# ==============================================================================
# BLOK 2: UI A HELPER FUNKCE
# ==============================================================================

def ai_box(content: str, title: str = "ü§ñ BMC Navig√°tor"):
    html_content = f"""<div style="border: 2px solid #4A90E2; border-radius: 10px; padding: 15px; margin: 10px 0; background-color: #F0F7FF; box-shadow: 2px 2px 5px rgba(0,0,0,0.1);"><p style="margin: 0; padding: 0; font-weight: bold; color: #4A90E2; font-family: sans-serif;">{title}</p><hr style="border: 0; border-top: 1px solid #D0E0F0; margin: 10px 0;"><p style="margin: 0; padding: 0; font-family: sans-serif; color: #333; line-height: 1.5;">{content}</p></div>"""
    st.markdown(html_content, unsafe_allow_html=True)

def user_response_box(response: str):
    html_content = f"""<div style="border: 2px solid #66BB6A; border-radius: 10px; padding: 15px; margin: 10px 0 10px 50px; background-color: #E8F5E9; box-shadow: 2px 2px 5px rgba(0,0,0,0.1); text-align: left;"><p style="margin: 0; padding: 0; font-weight: bold; color: #2E7D32; font-family: sans-serif;">Odpovƒõdƒõli jste</p><hr style="border: 0; border-top: 1px solid #C8E6C9; margin: 10px 0;"><p style="margin: 0; padding: 0; font-family: sans-serif; color: #333; line-height: 1.5;"><i>‚Äû{response}‚Äú</i></p></div>"""
    st.markdown(html_content, unsafe_allow_html=True)

def ask_gemini_sdk_st(prompt_text: str, temperature: float = None) -> str:
    if not model: return "AI_ERROR: Model nen√≠ inicializov√°n."
    config_overrides = {}
    if temperature is not None: config_overrides['temperature'] = float(temperature)
    spinner_text = "AI p≈ôem√Ω≈°l√≠..."
    if temperature: spinner_text += f" (teplota: {temperature})"
    with st.spinner(spinner_text):
        try:
            response = model.generate_content(prompt_text, generation_config=config_overrides)
            return response.text.strip()
        except Exception as e: st.error(f"CHYBA p≈ôi vol√°n√≠ API: {e}"); return f"AI_ERROR: Neoƒçek√°van√° chyba: {type(e).__name__}."

# ==============================================================================
# BLOK 3: PROMPTY
# ==============================================================================
LLM_EXPERT_QUESTION_PLANNER = """Jste expert na strategick√© poradenstv√≠ a mistr metodologie Business Model Canvas. Va≈°√≠m √∫kolem je vytvo≈ôit strukturovan√Ω a komplexn√≠ pl√°n dotazov√°n√≠ v **ƒçe≈°tinƒõ**. Tento pl√°n provede u≈æivatele popisem jeho IT byznysu. D≈ÆLE≈ΩIT√â: Vezmƒõte v √∫vahu **√∫vodn√≠ kontext**, kter√Ω u≈æivatel poskytl. V√°≈° v√Ωstup MUS√ç b√Ωt platn√Ω JSON seznam 9 objekt≈Ø. Ka≈æd√Ω objekt mus√≠ m√≠t **anglick√©** kl√≠ƒçe: "key", "question", "coverage_points" a "examples". D≈ÆLE≈ΩIT√â POKYNY PRO FORM√ÅTOV√ÅN√ç: "coverage_points" a "examples" mus√≠ b√Ωt seznamy plnohodnotn√Ωch vƒõt nebo fr√°z√≠. Ve≈°ker√Ω text v hodnot√°ch JSON MUS√ç b√Ωt v **ƒçe≈°tinƒõ**. Generujte POUZE JSON seznam."""
LLM_DEEP_ANALYSIS_PERSONA_V2 = """Jste strategick√Ω konzultant na √∫rovni partnera. Va≈°√≠m √∫kolem je prov√©st d≈Økladnou strategickou anal√Ωzu poskytnut√©ho Business Model Canvas (BMC) a vz√≠t v √∫vahu **√∫vodn√≠ kontext** u≈æivatele. Va≈°e anal√Ωza mus√≠ b√Ωt strukturov√°na: 1. Shrnut√≠ pro veden√≠. 2. Hloubkov√° anal√Ωza (SWOT s N√°lezem, D≈Økazem a Dopadem). 3. Kl√≠ƒçov√© souvislosti. 4. Kl√≠ƒçov√© strategick√© ot√°zky pro veden√≠. Buƒète d≈Økladn√Ω a profesion√°ln√≠. Odpov√≠dejte v **ƒçe≈°tinƒõ**."""
LLM_INNOVATION_LIST_GENERATOR = """Jste expert na obchodn√≠ inovace. Na z√°kladƒõ BMC a anal√Ωzy vygenerujte struƒçn√Ω, ƒç√≠slovan√Ω seznam **n√°zv≈Ø** inovativn√≠ch n√°pad≈Ø. N√°pady rozdƒõlte do kategori√≠: "Rychl√° v√≠tƒõzstv√≠", "Strategick√© posuny" a "Experiment√°ln√≠ n√°pady". Uveƒète pouze n√°zvy, ≈æ√°dn√© dal≈°√≠ detaily. Form√°tujte jako ƒç√≠slovan√Ω seznam. Odpov√≠dejte v **ƒçe≈°tinƒõ**."""
LLM_INNOVATION_DETAIL_GENERATOR = """Jste expert na obchodn√≠ inovace. Nyn√≠ detailnƒõ rozpracujte **jeden konkr√©tn√≠ n√°pad** na inovaci, jeho≈æ n√°zev je uveden n√≠≈æe. Pou≈æijte tento striktn√≠ form√°t pro svou odpovƒõƒè: **N√°zev n√°vrhu:**, **Popis:**, **Od≈Øvodnƒõn√≠ a napojen√≠ na anal√Ωzu:**, **Dopad na Business Model Canvas:**, **Akƒçn√≠ prvn√≠ kroky (p≈ô√≠≈°t√≠ch 30 dn√≠):** a **Mo≈æn√° rizika ke zv√°≈æen√≠:**. Buƒète maxim√°lnƒõ detailn√≠, konkr√©tn√≠ a akƒçn√≠. Odpov√≠dejte v **ƒçe≈°tinƒõ**."""

# ==============================================================================
# BLOK 4: HLAVN√ç TOK A UI APLIKACE
# ==============================================================================

# Inicializace Session State
if 'stage' not in st.session_state:
    st.session_state.stage = 'WELCOME'
    st.session_state.history = []
    st.session_state.user_context = ""
    st.session_state.question_plan = []
    st.session_state.current_question_index = 0
    st.session_state.bmc_data = {}
    st.session_state.analysis_result = ""
    st.session_state.innovation_titles = []

st.title("ü§ñ BMC Navig√°tor")
st.markdown("V√°≈° AI byznys strat√©g pro anal√Ωzu a inovaci va≈°eho byznys modelu.")

# Vykreslen√≠ historie konverzace
for item in st.session_state.history:
    if item['role'] == 'ai_question':
        ai_box(item['content'], item['title'])
    elif item['role'] == 'user_response':
        user_response_box(item['content'])
    elif item['role'] == 'llm_output':
        st.markdown(f"### {item['title']}")
        st.markdown(item['content'], unsafe_allow_html=True)
        st.markdown("---")

# Logika ≈ô√≠zen√≠ f√°z√≠ konverzace
if st.session_state.stage == 'WELCOME':
    ai_box("V√≠tejte! Ne≈æ zaƒçneme, popi≈°te pros√≠m va≈°i firmu, jej√≠ souƒçasn√Ω byznys model a p≈ô√≠padn√Ω sc√©n√°≈ô, kter√Ω chcete ≈ôe≈°it (nap≈ô. expanze, zmƒõna modelu).", "üöÄ V√≠tejte")
    context = st.text_area("V√°≈° popis:", height=150, key="context_input")
    if st.button("Potvrdit a zah√°jit anal√Ωzu"):
        if context:
            st.session_state.user_context = context
            st.session_state.history.append({'role': 'ai_question', 'title': 'üöÄ V√≠tejte', 'content': 'V√≠tejte! Ne≈æ zaƒçneme...'})
            st.session_state.history.append({'role': 'user_response', 'content': context})
            st.session_state.stage = 'PLAN_GENERATION'
            st.rerun()
        else:
            st.warning("Pros√≠m, zadejte popis va≈°√≠ firmy.")

elif st.session_state.stage == 'PLAN_GENERATION':
    with st.spinner("AI analyzuje v√°≈° kontext a p≈ôipravuje pl√°n dotazov√°n√≠..."):
        prompt_with_context = f"{LLM_EXPERT_QUESTION_PLANNER}\n\n√övodn√≠ popis od u≈æivatele:\n{st.session_state.user_context}\n---"
        response_text = ask_gemini_sdk_st(prompt_with_context, temperature=0.2)
    if "AI_ERROR" not in response_text:
        try:
            st.session_state.question_plan = json.loads(response_text.strip().lstrip("```json").rstrip("```").strip())
            st.session_state.stage = 'DATA_GATHERING'
            st.rerun()
        except Exception as e: st.error(f"Nepoda≈ôilo se zpracovat pl√°n od AI: {e}")
    else: st.error(response_text)

elif st.session_state.stage == 'DATA_GATHERING':
    q_index = st.session_state.current_question_index
    if q_index < len(st.session_state.question_plan):
        q_config = st.session_state.question_plan[q_index]
        q_title = f"Oblast {q_index+1}: {q_config.get('key', '').replace('_', ' ').title()}"
        q_text = q_config.get('question', 'Chyb√≠ text ot√°zky.')
        
        # Zobrazen√≠ ot√°zky (ale jen pokud je≈°tƒõ nebyla zobrazena v historii)
        if not st.session_state.history or st.session_state.history[-1].get('title') != q_title:
             st.session_state.history.append({'role': 'ai_question', 'title': q_title, 'content': q_text})
             st.rerun()
             
        # Zobrazen√≠ podot√°zek a p≈ô√≠klad≈Ø, kter√© nejsou v historii, jen jako n√°povƒõda
        st.markdown("**Pro komplexn√≠ odpovƒõƒè zva≈æte:**")
        for point in q_config.get('coverage_points', []): st.markdown(f"- {point}")
        st.markdown(f"**P≈ô√≠klady:** {', '.join(q_config.get('examples', []))}")
        
        answer = st.text_area("Va≈°e odpovƒõƒè:", height=200, key=f"answer_{q_index}")
        if st.button("Ulo≈æit a pokraƒçovat", key=f"submit_{q_index}"):
            st.session_state.bmc_data[q_config['key']] = answer
            st.session_state.history.append({'role': 'user_response', 'content': answer})
            st.session_state.current_question_index += 1
            st.rerun()
    else:
        st.session_state.stage = 'ANALYSIS'
        st.rerun()

elif st.session_state.stage == 'ANALYSIS':
    ai_box("Skvƒõle, m√°me zmapovan√Ω cel√Ω byznys model! Nyn√≠ provedu hloubkovou strategickou anal√Ωzu.", "üéâ Sbƒõr dat dokonƒçen")
    with st.spinner("AI prov√°d√≠ hloubkovou strategickou anal√Ωzu..."):
        bmc_data_string = "\n".join([f"- {key}: {value}" for key, value in st.session_state.bmc_data.items()])
        analysis_prompt = f"{LLM_DEEP_ANALYSIS_PERSONA_V2}\n\n√övodn√≠ kontext od u≈æivatele:\n{st.session_state.user_context}\n\nDetailn√≠ data z Business Model Canvas:\n{bmc_data_string}"
        analysis = ask_gemini_sdk_st(analysis_prompt, temperature=0.8)
    st.session_state.analysis_result = analysis
    st.session_state.history.append({'role': 'llm_output', 'title': 'F√°ze 3: Strategick√° anal√Ωza', 'content': analysis})
    st.session_state.stage = 'SUGGESTION_LIST'
    st.rerun()

elif st.session_state.stage == 'SUGGESTION_LIST':
    ai_box("Na z√°kladƒõ anal√Ωzy nyn√≠ vygeneruji nƒõkolik smƒõr≈Ø pro inovaci. Nejprve uvid√≠te jejich p≈ôehled a pot√© ka≈ædou rozpracuji do detailu.", "üí° F√°ze inovac√≠")
    with st.spinner("AI generuje p≈ôehled inovativn√≠ch n√°pad≈Ø..."):
        bmc_summary_str = "\n".join([f"- {k}: {v}" for k, v in st.session_state.bmc_data.items()])
        list_prompt = f"{LLM_INNOVATION_LIST_GENERATOR}\n\nKontext:\nPoƒç√°teƒçn√≠ c√≠l u≈æivatele:\n{st.session_state.user_context}\n\nBMC u≈æivatele:\n{bmc_summary_str}\n\nShrnut√≠ anal√Ωzy:\n{st.session_state.analysis_result}\n\nNyn√≠ vygenerujte struƒçn√Ω ƒç√≠slovan√Ω seznam n√°zv≈Ø inovac√≠."
        innovation_list_str = ask_gemini_sdk_st(list_prompt, temperature=1.2)
    st.session_state.innovation_titles = re.findall(r'^\s*\d+\.\s*(.*)', innovation_list_str, re.MULTILINE)
    st.session_state.history.append({'role': 'llm_output', 'title': 'P≈ôehled n√°vrh≈Ø inovac√≠', 'content': innovation_list_str})
    st.session_state.stage = 'SUGGESTION_DETAILS'
    st.rerun()

elif st.session_state.stage == 'SUGGESTION_DETAILS':
    ai_box(f"Nyn√≠ detailnƒõ rozpracuji tƒõchto {len(st.session_state.innovation_titles)} n√°pad≈Ø.", "Detailn√≠ rozpracov√°n√≠")
    bmc_summary_str = "\n".join([f"- {k}: {v}" for k, v in st.session_state.bmc_data.items()])
    all_details = ""
    for title in st.session_state.innovation_titles:
        with st.spinner(f"Rozpracov√°v√°m n√°pad: '{title.strip()}'..."):
            detail_prompt = f"{LLM_INNOVATION_DETAIL_GENERATOR}\n\nN√°zev n√°padu k rozpracov√°n√≠: '{title.strip()}'\n\nPodp≈Ørn√Ω kontext:\n{st.session_state.user_context}\n\nBMC:\n{bmc_summary_str}\n\nAnal√Ωza:\n{st.session_state.analysis_result}"
            detailed_suggestion = ask_gemini_sdk_st(detail_prompt, temperature=0.9)
        all_details += f"### Detail n√°vrhu: {title.strip()}\n{detailed_suggestion}\n\n---\n\n"
    st.session_state.history.append({'role': 'llm_output', 'title': 'Podrobn√© n√°vrhy inovac√≠', 'content': all_details})
    st.session_state.stage = 'FINISHED'
    st.rerun()

elif st.session_state.stage == 'FINISHED':
    ai_box("T√≠mto konƒç√≠ na≈°e interaktivn√≠ sezen√≠. Pro zah√°jen√≠ nov√© anal√Ωzy obnovte str√°nku (F5).", title="üéâ Sezen√≠ dokonƒçeno")
