# ==============================================================================
# BLOK 1: INSTALACE A KONFIGURACE
# ==============================================================================
# Instalace pot≈ôebn√Ωch knihoven
!pip install -q google-generativeai
!pip install -q python-dotenv

import os
import time
import google.generativeai as genai
import json
from IPython.display import display, HTML
import textwrap
import re

# --- Konfigurace API kl√≠ƒçe ---
try:
    from google.colab import userdata
    GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')
    print("API kl√≠ƒç √∫spƒõ≈°nƒõ naƒçten z Google Colab Secrets.")
except ImportError:
    print("Nelze naƒç√≠st z Colab Secrets. Ujistƒõte se, ≈æe spou≈°t√≠te v Google Colabu.")
    GOOGLE_API_KEY = None

if not GOOGLE_API_KEY:
    raise ValueError("Google API Key nen√≠ nastaven. V Google Colabu jej nastavte v sekci 'Tajemstv√≠' (Secrets) s n√°zvem 'GOOGLE_API_KEY'.")
else:
    genai.configure(api_key=GOOGLE_API_KEY)
    print("Google Generative AI API konfigurov√°no.")

# --- Konfigurace Modelu ---
PRIORITY_MODEL_STEMS = [
    "gemini-2.5-flash-preview-05-20", # V√ÅMI PO≈ΩADOVAN√ù MODEL
    "gemini-1.5-flash-latest",
    "gemini-1.5-pro-latest",
    "gemini-pro",
]
GENERATION_CONFIG = {
    "temperature": 0.7,
    "top_p": 0.95,
    "max_output_tokens": 65536,
}
model = None

try:
    print("\nHled√°m dostupn√Ω Gemini model...")
    model_name_to_use = None
    available_models = [m for m in genai.list_models() if "generateContent" in m.supported_generation_methods]

    for model_stem in PRIORITY_MODEL_STEMS:
        found_model = next((m for m in available_models if model_stem in m.name and 'vision' not in m.name.lower()), None)
        if found_model:
            model_name_to_use = found_model.name
            print(f"  > Nalezen prioritn√≠ model: {model_name_to_use}")
            break

    if not model_name_to_use:
        raise ValueError("Nebyl nalezen ≈æ√°dn√Ω z prioritn√≠ch model≈Ø. Zkontrolujte dostupnost model≈Ø ve va≈°em regionu.")

    model = genai.GenerativeModel(
        model_name=model_name_to_use,
        generation_config=GENERATION_CONFIG
    )
    print(f"Model '{model_name_to_use}' √∫spƒõ≈°nƒõ inicializov√°n.")
except Exception as e:
    print(f"KRITICK√Å CHYBA p≈ôi inicializaci modelu: {e}")


# ==============================================================================
# BLOK 2: UI A HELPER FUNKCE (V≈†E V ƒåE≈†TINƒö)
# ==============================================================================

def wrap_text(text, width=110):
    return '<br>'.join(textwrap.wrap(text, width=width, replace_whitespace=False))

def ai_box(content: str, title: str = "ü§ñ BMC Navig√°tor"):
    wrapped_content = wrap_text(content)
    html_content = f"""<div style="border: 2px solid #4A90E2; border-radius: 10px; padding: 15px; margin: 10px 0; background-color: #F0F7FF; box-shadow: 2px 2px 5px rgba(0,0,0,0.1);"><p style="margin: 0; padding: 0; font-weight: bold; color: #4A90E2; font-family: sans-serif;">{title}</p><hr style="border: 0; border-top: 1px solid #D0E0F0; margin: 10px 0;"><p style="margin: 0; padding: 0; font-family: sans-serif; color: #333; line-height: 1.5;">{wrapped_content}</p></div>"""
    display(HTML(html_content))

def user_prompt_box(prompt_text: str) -> str:
    html_prompt = f"""<div style="border: 1px solid #50a14f; border-radius: 5px; padding: 10px; margin: 10px 0; background-color: #F7FFF7;"><p style="margin: 0; padding: 0; font-weight: bold; color: #387038; font-family: sans-serif;">‚úçÔ∏è √ökol pro v√°s</p><p style="margin: 0; padding: 5px 0 0 0; font-family: sans-serif; color: #333;">{prompt_text}</p></div>"""
    display(HTML(html_prompt))
    return input("Va≈°e odpovƒõƒè > ")

def display_user_response(response: str):
    wrapped_response = wrap_text(response)
    html_content = f"""<div style="border: 2px solid #66BB6A; border-radius: 10px; padding: 15px; margin: 10px 0 10px 50px; background-color: #E8F5E9; box-shadow: 2px 2px 5px rgba(0,0,0,0.1); text-align: left;"><p style="margin: 0; padding: 0; font-weight: bold; color: #2E7D32; font-family: sans-serif;">Odpovƒõdƒõli jste</p><hr style="border: 0; border-top: 1px solid #C8E6C9; margin: 10px 0;"><p style="margin: 0; padding: 0; font-family: sans-serif; color: #333; line-height: 1.5;"><i>‚Äû{wrapped_response}‚Äú</i></p></div>"""
    display(HTML(html_content))

def display_status_message(message: str):
    html_content = f"""<p style="font-family: sans-serif; color: #888; font-style: italic; text-align: center; margin: 10px 0;">{message}</p>"""
    display(HTML(html_content))

def display_llm_output(title: str, content: str):
    from IPython.display import Markdown
    print(f"\n--- {title} ---")
    if "AI_ERROR" in content:
        ai_box(content, title="‚ùå Nastala chyba")
    else:
        display(Markdown(content))
    print("--------------------------------------------------")

def ask_gemini_sdk(prompt_text: str, temperature: float = None) -> str:
    if not model: return "AI_ERROR: Model nen√≠ inicializov√°n."
    config_overrides = {}
    if temperature is not None:
        config_overrides['temperature'] = float(temperature)
        display_status_message(f"AI p≈ôem√Ω≈°l√≠ s teplotou: {config_overrides['temperature']}...")
    else:
        display_status_message("AI p≈ôem√Ω≈°l√≠ s v√Ωchoz√≠ teplotou...")
    try:
        response = model.generate_content(prompt_text, generation_config=config_overrides)
        if response.parts: return response.text.strip()
        elif response.prompt_feedback: return f"AI_ERROR: Po≈æadavek byl zablokov√°n ({response.prompt_feedback.block_reason.name})."
        else: return "AI_ERROR: Obdr≈æena nekompletn√≠ odpovƒõƒè od modelu."
    except Exception as e:
        display_status_message(f"CHYBA p≈ôi vol√°n√≠ API: {e}")
        return f"AI_ERROR: Neoƒçek√°van√° chyba: {type(e).__name__}."

# ==============================================================================
# BLOK 3: PROMPTY A LOGIKA APLIKACE
# ==============================================================================

LLM_EXPERT_QUESTION_PLANNER = """Jste expert na strategick√© poradenstv√≠ a mistr metodologie Business Model Canvas. Va≈°√≠m √∫kolem je vytvo≈ôit strukturovan√Ω a komplexn√≠ pl√°n dotazov√°n√≠ v **ƒçe≈°tinƒõ**. Tento pl√°n provede u≈æivatele popisem jeho IT byznysu. D≈ÆLE≈ΩIT√â: Vezmƒõte v √∫vahu **√∫vodn√≠ kontext**, kter√Ω u≈æivatel poskytl. V√°≈° v√Ωstup MUS√ç b√Ωt platn√Ω JSON seznam 9 objekt≈Ø. Ka≈æd√Ω objekt mus√≠ m√≠t **anglick√©** kl√≠ƒçe: "key", "question", "coverage_points" a "examples". D≈ÆLE≈ΩIT√â POKYNY PRO FORM√ÅTOV√ÅN√ç: "coverage_points" a "examples" mus√≠ b√Ωt seznamy plnohodnotn√Ωch vƒõt nebo fr√°z√≠. Ve≈°ker√Ω text v hodnot√°ch JSON MUS√ç b√Ωt v **ƒçe≈°tinƒõ**. Generujte POUZE JSON seznam."""
LLM_DEEP_ANALYSIS_PERSONA_V2 = """Jste strategick√Ω konzultant na √∫rovni partnera. Va≈°√≠m √∫kolem je prov√©st d≈Økladnou strategickou anal√Ωzu poskytnut√©ho Business Model Canvas (BMC) a vz√≠t v √∫vahu **√∫vodn√≠ kontext** u≈æivatele. Va≈°e anal√Ωza mus√≠ b√Ωt strukturov√°na: 1. Shrnut√≠ pro veden√≠. 2. Hloubkov√° anal√Ωza (SWOT s N√°lezem, D≈Økazem a Dopadem). 3. Kl√≠ƒçov√© souvislosti. 4. Kl√≠ƒçov√© strategick√© ot√°zky pro veden√≠. Buƒète d≈Økladn√Ω a profesion√°ln√≠. Odpov√≠dejte v **ƒçe≈°tinƒõ**."""
LLM_INNOVATION_LIST_GENERATOR = """Jste expert na obchodn√≠ inovace. Na z√°kladƒõ BMC a anal√Ωzy vygenerujte struƒçn√Ω, ƒç√≠slovan√Ω seznam **n√°zv≈Ø** inovativn√≠ch n√°pad≈Ø. N√°pady rozdƒõlte do kategori√≠: "Rychl√° v√≠tƒõzstv√≠", "Strategick√© posuny" a "Experiment√°ln√≠ n√°pady". Uveƒète pouze n√°zvy, ≈æ√°dn√© dal≈°√≠ detaily. Form√°tujte jako ƒç√≠slovan√Ω seznam. Odpov√≠dejte v **ƒçe≈°tinƒõ**."""
LLM_INNOVATION_DETAIL_GENERATOR = """Jste expert na obchodn√≠ inovace. Nyn√≠ detailnƒõ rozpracujte **jeden konkr√©tn√≠ n√°pad** na inovaci, jeho≈æ n√°zev je uveden n√≠≈æe. Pou≈æijte tento striktn√≠ form√°t pro svou odpovƒõƒè: **N√°zev n√°vrhu:**, **Popis:**, **Od≈Øvodnƒõn√≠ a napojen√≠ na anal√Ωzu:**, **Dopad na Business Model Canvas:**, **Akƒçn√≠ prvn√≠ kroky (p≈ô√≠≈°t√≠ch 30 dn√≠):** a **Mo≈æn√° rizika ke zv√°≈æen√≠:**. Buƒète maxim√°lnƒõ detailn√≠, konkr√©tn√≠ a akƒçn√≠. Odpov√≠dejte v **ƒçe≈°tinƒõ**."""

def get_initial_user_context() -> str:
    prompt_text = "Ne≈æ zaƒçneme, popi≈°te pros√≠m voln√Ωm textem va≈°i firmu, jej√≠ souƒçasn√Ω byznys model a p≈ô√≠padn√Ω sc√©n√°≈ô, kter√Ω chcete ≈ôe≈°it (nap≈ô. expanze, zmƒõna modelu). ƒå√≠m v√≠ce kontextu mi d√°te, t√≠m relevantnƒõj≈°√≠ bude na≈°e dal≈°√≠ pr√°ce."
    user_response = user_prompt_box(prompt_text)
    display_user_response(user_response)
    ai_box("Dƒõkuji za kontext. Nyn√≠ p≈ôiprav√≠m pl√°n dotazov√°n√≠ na m√≠ru va≈°√≠ situaci.", title="‚úÖ Kontext p≈ôijat")
    return user_response

def get_user_input_with_llm_validation(bmc_block_question: str, block_name: str, coverage_points: list = None, it_examples: list[str] = None) -> str:
    full_prompt_html = f"<p style='font-size: 1.1em; margin-bottom: 15px;'>{bmc_block_question}</p>"
    if coverage_points:
        full_prompt_html += "<p style='margin-bottom: 5px; font-weight: bold;'>Pro komplexn√≠ odpovƒõƒè zva≈æte pros√≠m n√°sleduj√≠c√≠ body:</p><ul style='margin-top: 5px; margin-left: 20px;'>"
        for point in coverage_points: full_prompt_html += f"<li style='margin-bottom: 5px;'>{point}</li>"
        full_prompt_html += "</ul>"
    if it_examples:
        full_prompt_html += f"<p style='margin-top: 15px; font-style: italic; color: #555;'>Nap≈ô√≠klad: {', '.join(it_examples)}.</p>"
    user_response = user_prompt_box(full_prompt_html)
    user_response_stripped = user_response.strip()
    display_user_response(user_response_stripped)
    if user_response_stripped.lower() in ["n/a", "ne", "p≈ôeskoƒçit", "skip"]:
        ai_box(f"Rozum√≠m. P≈ôeskoƒç√≠me oblast '{block_name}'.", title="‚úÖ Potvrzeno")
        return "Skipped"
    return user_response_stripped

def generate_question_plan(user_context: str) -> list:
    ai_box("Na z√°kladƒõ va≈°eho popisu p≈ôipravuji personalizovan√Ω pl√°n dotazov√°n√≠...", title="üß† P≈ô√≠prava pl√°nu")
    prompt_with_context = f"{LLM_EXPERT_QUESTION_PLANNER}\n\n√övodn√≠ popis od u≈æivatele (pou≈æijte jako kontext):\n---\n{user_context}\n---"
    response_text = ask_gemini_sdk(prompt_with_context, temperature=0.2)
    if "AI_ERROR" in response_text:
        ai_box("Nepoda≈ôilo se mi vytvo≈ôit pl√°n. Zkuste pros√≠m spustit bu≈àku znovu.", title="‚ùå Chyba pl√°nu")
        return []
    try:
        cleaned_json_text = response_text.strip().lstrip("```json").rstrip("```").strip()
        question_plan = json.loads(cleaned_json_text)
        if isinstance(question_plan, list) and all('key' in item for item in question_plan):
            ai_box(f"Pl√°n dotazov√°n√≠ byl √∫spƒõ≈°nƒõ vygenerov√°n. Zept√°m se v√°s na {len(question_plan)} kl√≠ƒçov√Ωch oblast√≠.", title="‚úÖ Pl√°n p≈ôipraven")
            return question_plan
        else: raise ValueError("Vygenerovan√Ω JSON postr√°d√° po≈æadovan√© kl√≠ƒçe.")
    except (json.JSONDecodeError, ValueError) as e:
        ai_box(f"Nastala chyba p≈ôi zpracov√°n√≠ vygenerovan√©ho pl√°nu: {e}.", title="‚ùå Chyba zpracov√°n√≠")
        return []

def conduct_dynamic_bmc_analysis(question_plan: list) -> dict:
    ai_box("Nyn√≠ spoleƒçnƒõ projdeme jednotliv√© bloky va≈°eho byznys modelu do hloubky.", title="üöÄ Jdeme na to")
    bmc_data = {}
    for i, config in enumerate(question_plan):
        display_status_message(f"Oblast {i+1} z {len(question_plan)}: {config.get('key', 'Nezn√°m√Ω blok').replace('_', ' ').title()}")
        response = get_user_input_with_llm_validation(
            bmc_block_question=config.get('question', 'Chyb√≠ text ot√°zky.'),
            block_name=config.get('key', f'Ot√°zka {i+1}'),
            coverage_points=config.get('coverage_points', []),
            it_examples=config.get('examples', [])
        )
        bmc_data[config.get('key', f'custom_question_{i+1}')] = response
    ai_box("Skvƒõl√° pr√°ce! Zmapovali jsme cel√Ω v√°≈° byznys model.", title="üéâ Hotovo")
    return bmc_data

def perform_llm_bmc_analysis(bmc_data: dict, user_context: str) -> str:
    display_status_message("Zahajuji expertn√≠ strategickou anal√Ωzu...")
    bmc_data_string = "\n".join([f"- {key}: {value}" for key, value in bmc_data.items() if value != "Skipped"])
    analysis_prompt = f"{LLM_DEEP_ANALYSIS_PERSONA_V2}\n\n√övodn√≠ kontext od u≈æivatele:\n{user_context}\n\nDetailn√≠ data z Business Model Canvas:\n{bmc_data_string}"
    return ask_gemini_sdk(analysis_prompt, temperature=0.8)

def generate_innovation_list(bmc_data_str: str, analysis_result: str, user_context: str) -> str:
    display_status_message("Generuji p≈ôehled inovativn√≠ch n√°pad≈Ø...")
    list_prompt = f"{LLM_INNOVATION_LIST_GENERATOR}\n\nKontext:\nPoƒç√°teƒçn√≠ c√≠l u≈æivatele:\n{user_context}\n\nBMC u≈æivatele:\n{bmc_data_str}\n\nShrnut√≠ anal√Ωzy:\n{analysis_result}\n\nNyn√≠ vygenerujte struƒçn√Ω ƒç√≠slovan√Ω seznam n√°zv≈Ø inovac√≠."
    return ask_gemini_sdk(list_prompt, temperature=1.2)

def generate_detailed_innovation(innovation_title: str, bmc_data_str: str, analysis_result: str, user_context: str) -> str:
    display_status_message(f"Rozpracov√°v√°m detailnƒõ n√°pad: '{innovation_title}'...")
    detail_prompt = f"{LLM_INNOVATION_DETAIL_GENERATOR}\n\nN√°zev n√°padu k rozpracov√°n√≠: '{innovation_title}'\n\nPodp≈Ørn√Ω kontext:\nPoƒç√°teƒçn√≠ c√≠l u≈æivatele:\n{user_context}\n\nBMC u≈æivatele:\n{bmc_data_str}\n\nShrnut√≠ anal√Ωzy:\n{analysis_result}\n\nNyn√≠ detailnƒõ rozpracujte tento jeden n√°pad podle zadan√©ho form√°tu."
    return ask_gemini_sdk(detail_prompt, temperature=0.9)

# ==============================================================================
# BLOK 4: HLAVN√ç SPU≈†TƒöC√ç FUNKCE
# ==============================================================================

def run_main_session():
    """Orchestruje cel√© sezen√≠ s BMC Navig√°torem."""
    if not model:
        ai_box("Gemini model nebyl inicializov√°n. Sezen√≠ nem≈Ø≈æe zaƒç√≠t.", title="Kritick√° chyba")
        return

    # F√°ze 1: Uv√≠t√°n√≠ a z√≠sk√°n√≠ kontextu
    ai_box("V√≠tejte v BMC Navig√°toru! Jsem v√°≈° AI byznys strat√©g p≈ôipraven√Ω pomoci v√°m analyzovat a inovovat v√°≈° byznys model.", title="üöÄ V√≠tejte")
    user_context = get_initial_user_context()
    if not user_context:
        ai_box("Bez √∫vodn√≠ho popisu nem≈Ø≈æeme pokraƒçovat. Zkuste pros√≠m spustit sezen√≠ znovu.", title="‚ùå Chyb√≠ kontext")
        return

    # F√°ze 2: Dotazov√°n√≠
    question_plan = generate_question_plan(user_context)
    if not question_plan:
        ai_box("Nepoda≈ôilo se mi p≈ôipravit pl√°n dotazov√°n√≠. Zkuste pros√≠m spustit sezen√≠ znovu.", title="‚ùå Chyba spu≈°tƒõn√≠")
        return
    current_bmc_data = conduct_dynamic_bmc_analysis(question_plan)

    # F√°ze 3: Anal√Ωza
    analysis_result = perform_llm_bmc_analysis(current_bmc_data, user_context)
    display_llm_output("F√°ze 3: Strategick√° anal√Ωza", analysis_result)

    # F√°ze 4: Inovace (dvouf√°zov√°)
    ai_box("Na z√°kladƒõ anal√Ωzy nyn√≠ vygeneruji nƒõkolik smƒõr≈Ø pro inovaci va≈°eho byznys modelu. Nejprve uvid√≠te jejich p≈ôehled a pot√© ka≈ædou rozpracuji do detailu.", title="üí° F√°ze inovac√≠")
    bmc_summary_str = "\n".join([f"- {k}: {v}" for k, v in current_bmc_data.items() if v != "Skipped"])

    # Krok 4a: Z√≠sk√°n√≠ seznamu inovac√≠
    innovation_list_str = generate_innovation_list(bmc_summary_str, analysis_result, user_context)
    display_llm_output("P≈ôehled n√°vrh≈Ø inovac√≠", innovation_list_str)

    # Krok 4b: Rozpracov√°n√≠ ka≈æd√© inovace
    innovation_titles = re.findall(r'^\s*\d+\.\s*(.*)', innovation_list_str, re.MULTILINE)
    if not innovation_titles:
        ai_box("Nepoda≈ôilo se mi extrahovat n√°zvy inovac√≠ ze seznamu. Pokraƒçov√°n√≠ nen√≠ mo≈æn√©.", title="Chyba zpracov√°n√≠")
        return

    ai_box(f"Nyn√≠ detailnƒõ rozpracuji tƒõchto {len(innovation_titles)} n√°pad≈Ø.", title="Detailn√≠ rozpracov√°n√≠")
    for title in innovation_titles:
        # OPRAVENO: Pou≈æita spr√°vn√° promƒõnn√° 'analysis_result' m√≠sto neexistuj√≠c√≠ 'analysis_summary'
        detailed_suggestion = generate_detailed_innovation(title.strip(), bmc_summary_str, analysis_result, user_context)
        display_llm_output(f"Detail n√°vrhu: {title.strip()}", detailed_suggestion)
        time.sleep(1)

    # Z√°vƒõr
    ai_box("T√≠mto konƒç√≠ na≈°e interaktivn√≠ sezen√≠. Douf√°m, ≈æe detailn√≠ anal√Ωza a n√°vrhy byly p≈ô√≠nosn√© pro va≈°e strategick√© pl√°nov√°n√≠.", title="üéâ Sezen√≠ dokonƒçeno")

# --- Spu≈°tƒõn√≠ cel√© aplikace ---
if __name__ == "__main__":
    run_main_session()
